# server/api/llm.py
from __future__ import annotations
import os, json, re
from typing import Dict, Any, Optional
#For ollama
import requests 

DEBUG_LLM: bool = os.getenv("DEBUG_LLM") == "1"

def _log(msg: str) -> None:
    if DEBUG_LLM:
        print(f"[LLM] {msg}")

# Template when no llm avaliable
TEMPLATES: Dict[str, Dict[str, Any]] = {
    "email": {
        "regex": r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}",
        "explanation": "Matches most email addresses like name@domain.com",
        "confidence": 0.9,
    },
    "phone": {
        "regex": r"\b(?:\+?61[-\s]?4\d{2}[-\s]?\d{3}[-\s]?\d{3}|04\d{2}[-\s]?\d{3}[-\s]?\d{3})\b",
        "explanation": "AU-like phone numbers",
        "confidence": 0.6,
    },
    "date": {
        "regex": r"\b(?:\d{4}[-/]\d{1,2}[-/]\d{1,2}|\d{1,2}[-/]\d{1,2}[-/]\d{2,4})\b",
        "explanation": "Dates like 2024-01-31 or 01/31/2024",
        "confidence": 0.6,
    },
    "url": {
        "regex": r"https?://[^\s]+",
        "explanation": "Basic http/https URLs",
        "confidence": 0.75,
    },
    "number": {
        "regex": r"\d+",
        "explanation": "One or more digits",
        "confidence": 0.95,
    },
    "digits only": {
        "regex": r"^\d+$",
        "explanation": "For those cell which only contains digits",
        "confidence": 0.8,
    },
    "ip": {
        "regex": r"\b(?:\d{1,3}\.){3}\d{1,3}\b",
        "explanation": "IPv4",
        "confidence": 0.6,
    },
    "everything": {
        "regex": r"^.*$",
        "explanation": "Match entire cell",
        "confidence": 1.0,
    },
}

def _template_suggest(instruction: str) -> Dict[str, Any]:
    # If no llm, then use the hard-coded templates
    text = (instruction or "").lower()
    order = ["email", "phone", "date", "url", "digits only", "number", "ip", "everything"]
    for key in order:
        if key in text:
            out = TEMPLATES[key].copy()
            out["source"] = "template"  # 自创字段：来源说明
            return out
    out = TEMPLATES["everything"].copy()
    out["source"] = "template"
    return out


def _normalize_llm_json(s: str) -> Optional[Dict[str, Any]]:
    m = re.search(r"\{.*\}", s, re.S)
    if not m:
        return None
    try:
        return json.loads(m.group(0))
    except Exception:
        return None


# 1st try: if there is local llm (ollama is used here)
OLLAMA_BASE: str = "http://localhost:11434"

def _has_ollama() -> bool:
    try:
        url = f"{OLLAMA_BASE.rstrip('/')}/api/version"
        r = requests.get(url, timeout=1.5)
        _log(f"Probe Ollama {url} -> {r.status_code}")
        return r.ok
    except Exception as e:
        _log(f"Ollama probe failed: {repr(e)}")
        return False

def _ollama_suggest(instruction: str, column: str | None) -> Optional[Dict[str, Any]]:
    try:
        url = f"{OLLAMA_BASE.rstrip('/')}/api/generate"
        payload = {
            "model": "llama3.1:8b",
            "prompt": _prompt(instruction, column),
            "stream": False
        }
        r = requests.post(url, json=payload, timeout=60)
        r.raise_for_status()
        data = r.json()
        content = data.get("response", "")
        _log(f"Ollama content: {content[:200]}...")
        obj = _normalize_llm_json(content)
        if obj and isinstance(obj.get("regex"), str):
            return {
                "regex": obj["regex"],
                "explanation": obj.get("explanation", "Generated by LLM (local)"),
                "confidence": float(obj.get("confidence", 0.6)),
                "source": "ollama",
            }
    except Exception as e:
        _log(f"Ollama exception: {repr(e)}")
    return None

# 2nd try: if there is openai subs
def _openai_client():
    key = os.getenv("OPENAI_API_KEY") or ""
    if not key:
        _log("OPENAI_API_KEY missing")
        return None
    try:
        from openai import OpenAI
        return OpenAI(api_key=key)
    except Exception as e:
        _log(f"OpenAI import/init failed: {repr(e)}")
        return None

def _openai_suggest(client, instruction: str, column: str | None) -> Optional[Dict[str, Any]]:
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": _prompt(instruction, column)}],
            temperature=0.2,
        )
        content = resp.choices[0].message.content or ""
        _log(f"OpenAI content: {content[:200]}...")
        obj = _normalize_llm_json(content)
        if obj and isinstance(obj.get("regex"), str):
            return {
                "regex": obj["regex"],
                "explanation": obj.get("explanation", "Generated by LLM"),
                "confidence": float(obj.get("confidence", 0.7)),
                "source": "openai",
            }
    except Exception as e:
        _log(f"OpenAI exception: {repr(e)}")
    return None

# llm Prompt
def _prompt(instruction: str, column: str | None) -> str:
    return f"""
You write regex patterns for pandas.str.replace (Python).
User instruction: {'find' + instruction!r}
Target column (optional): {column!r}

Rules:
- Return ONE regex string only (no flags like /.../i).
- Keep it efficient (avoid catastrophic backtracking).
- If unsure, default to a simple but safe pattern.
- Output a single JSON object with: regex, explanation, confidence (0..1).
Example:
{{"regex":"\\\\d+","explanation":"digits","confidence":0.8}}
""".strip()

# output model
def suggest_regex(instruction: str, column: str | None = None) -> Dict[str, Any]:
    # logic: 
    # 1. check local: if local llm, then use local llm
    # 2. Check if there is Open AI Sub
    # 3. Use local template if nothing above 
    _log("== suggest_regex start ==")

    # 1. ollama
    if _has_ollama():
        res = _ollama_suggest(instruction, column)
        if res:
            _log("Using Ollama result")
            return res
        _log("Ollama failed, falling back")

    # 2. openai
    client = _openai_client()
    if client is not None:
        res = _openai_suggest(client, instruction, column)
        if res:
            _log("Using OpenAI result")
            return res
        _log("OpenAI failed or insufficient quota, falling back")

    # template
    _log("Using template fallback")
    return _template_suggest(instruction)
